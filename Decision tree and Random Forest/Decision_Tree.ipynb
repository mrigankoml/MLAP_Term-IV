{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision_Tree.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMCz2C34xWVN"
      },
      "source": [
        "**Understanding how a decision tree works**\n",
        "### A decision tree consists of creating different rules by which we make the prediction. For example, let’s say we train an algorithm that predicts whether or not a person is obese based on their height and weight. To do this, we will use the following dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "mzjYdzXfxa8Y",
        "outputId": "8fbfd9e4-02d2-4dd7-a324-6fedb246a0dc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#dataset_url = \"https://github.com/Balachandar-R/dataset/blob/main/500_Person_Gender_Height_Weight_Index.csv\"\n",
        "\n",
        "data = pd.read_csv('/content/sample_data/500_Person_Gender_Height_Weight_Index.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>174</td>\n",
              "      <td>96</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>189</td>\n",
              "      <td>87</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Female</td>\n",
              "      <td>185</td>\n",
              "      <td>110</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Female</td>\n",
              "      <td>195</td>\n",
              "      <td>104</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>149</td>\n",
              "      <td>61</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Gender  Height  Weight  Index\n",
              "0    Male     174      96      4\n",
              "1    Male     189      87      2\n",
              "2  Female     185     110      4\n",
              "3  Female     195     104      3\n",
              "4    Male     149      61      3"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgNpr4hy46b-"
      },
      "source": [
        "Imagine that we want to predict whether or not the person is obese. Based on the description of the dataset (available on Kaggle), people with an index of 4 or 5 are obese, so we could create a variable that reflects this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGivYVa_48CJ"
      },
      "source": [
        "data['obese'] = (data.Index >= 4).astype('int')\n",
        "data.drop('Index', axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukf-P3gL5KKg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JVhg5wW5GUW"
      },
      "source": [
        "In that case, a decision tree would tell us different rules, such as that if the person’s weight is greater than 100kg, it is most likely that the person is obese. However, that cut will not be precise: there will be people who weigh 100kg or more who are not obese. Thus, the decision tree continues to create more branches that generate new conditions to “refine” our predictions.\n",
        "\n",
        "As you can see, decision trees usually have sub-trees that serve to fine-tune the prediction of the previous node. This is so until we get to a node that does not split. This last node is known as a leaf node or leaf node. Let’s see a graphic example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMLGt85j6NA6"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTORLKE86Uqr"
      },
      "source": [
        "### Besides,a decision trees can work for both regression problems and for classification problems. In fact, we will code a decision tree from scratch that can do both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2YigIkx6vnz"
      },
      "source": [
        "**Impurity and cost functions of a decision tree**\n",
        "As in all algorithms, the cost function is the basis of the algorithm. In the case of decision trees, there are two main cost functions: the Gini index and entropy.\n",
        "\n",
        "Any of the cost functions we can use are based on measuring impurity. Impurity refers to the fact that, when we make a cut, how likely is it that the target variable will be classified incorrectly.\n",
        "\n",
        "**In the example above, impurity will include the percentage of people that weight >=100 kg that are not obese and the percentage of people with weight<100 kg that are obese. Every time we make a split and the classification is not perfect, the split is impure.**\n",
        "\n",
        "However, this does not mean that all cuts are the same: sure that the cut in 100kg classifies better than if we make the split at 80kg. In fact, we can check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewcW2nlP6zbA",
        "outputId": "87201ba6-d3a0-4252-a7f9-f05fd702dc4f"
      },
      "source": [
        "print(\n",
        "  \" Misclassified when cutting at 100kg:\",\n",
        "  data.loc[(data['Weight']>=100) & (data['obese']==0),:].shape[0], \"\\n\",\n",
        "  \"Misclassified when cutting at 80kg:\",\n",
        "  data.loc[(data['Weight']>=80) & (data['obese']==0),:].shape[0]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Misclassified when cutting at 100kg: 18 \n",
            " Misclassified when cutting at 80kg: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2R2xBTB7Hc4"
      },
      "source": [
        "In short, **the cost function of a decision tree seeks to find those cuts that minimize impurity.** Now, let’s see what ways exist to calculate impurity:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izRRZNmM7VWe"
      },
      "source": [
        "**Calculate impurity using the Gini index**\n",
        "\n",
        "The Gini index is the most widely used cost function in decision trees. This index calculates the amount of probability that a specific characteristic will be classified incorrectly when it is randomly selected.\n",
        "\n",
        "This is an index that ranges from 0 (a pure cut) to 0.5 (a completely pure cut that divides the data equally). The Gini index is calculated as follows:\n",
        "\n",
        "**𝐺𝑖𝑛𝑖=1–∑𝑖=1𝑛(𝑃𝑖)2**\n",
        "\n",
        "Where Pi is the probability of having that class or value.\n",
        "\n",
        "Let’s program the function, considering the input will be a Pandas series:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D5taSnE7twJ",
        "outputId": "12bee5c9-4301-4725-9202-a06177aa9005"
      },
      "source": [
        "def gini_impurity(y):\n",
        "  '''\n",
        "  Given a Pandas Series, it calculates the Gini Impurity. \n",
        "  y: variable with which calculate Gini Impurity.\n",
        "  '''\n",
        "  if isinstance(y, pd.Series):\n",
        "    p = y.value_counts()/y.shape[0]\n",
        "    gini = 1-np.sum(p**2)\n",
        "    return(gini)\n",
        "\n",
        "  else:\n",
        "    raise('Object must be a Pandas Series.')\n",
        "\n",
        "gini_impurity(data.Gender) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4998"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JPmxG7m8Fn3"
      },
      "source": [
        "# **Calculate impurity with entropy**\n",
        "\n",
        "Entropy it is a way of measuring impurity or randomness in data points. Entropy is defined by the following formula:\n",
        "\n",
        "**𝐸(𝑆)=∑𝑖=1𝑐−𝑝𝑖𝑙𝑜𝑔2𝑝𝑖**\n",
        "Unlike the Gini index, whose range goes from 0 to 0.5, the entropy range is different, since it goes from 0 to 1. In this way, values close to zero are less impure than those that approach 1.\n",
        "\n",
        "**Let’s see how entropy works by calculating it for the same example that we have done with the Gini index:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOpTkMbT8BbT"
      },
      "source": [
        "As we can see, **the Gini index for the Gender variable is very close to 0.5**. \n",
        "This indicates that the Gender variable is very impure, that is, the cutting results are not will both have equally the same proportion of incorrectly classified data.\n",
        "\n",
        "Now that you know how the index works, **let’s see how entropy works.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abj82iKq8vFe",
        "outputId": "7de0d0e7-8f6d-491f-9b1a-03b82fd48208"
      },
      "source": [
        "def entropy(y):\n",
        "  '''\n",
        "  Given a Pandas Series, it calculates the entropy. \n",
        "  y: variable with which calculate entropy.\n",
        "  '''\n",
        "  if isinstance(y, pd.Series):\n",
        "    a = y.value_counts()/y.shape[0]\n",
        "    entropy = np.sum(-a*np.log2(a+1e-9))\n",
        "    return(entropy)\n",
        "\n",
        "  else:\n",
        "    raise('Object must be a Pandas Series.')\n",
        "\n",
        "entropy(data.Gender) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9997114388674198"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47rFZzG86yj"
      },
      "source": [
        "**As we see, it gives us a value very close to 1, which denotes an impurity similar to that indicated by the Gini impurity, whose value is close to 0.5.**\n",
        "\n",
        "With this, you already know the two main methods that can be used in a decision tree to calculate impurity. Perfect, we already know how to decide if a cut is good or not, but… between which splits do we choose? Let’s see it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwWnu2Sp9otW"
      },
      "source": [
        "# **How to choose the cuts for our decision tree**\n",
        "As we have seen, cuts are compared by impurity. Therefore, we are interested in comparing those cuts that generate less impurity. For this, Information Gain is used. This metric indicates the improvement when making different partitions and is usually used with entropy (it could also be used with the Gini index, although in that case it would not be called Informaiton Gain).\n",
        "\n",
        "The calculation of the Information Gain will depend on whether it is a classification or regression decision tree. There would be two options:\n",
        "\n",
        "## 𝐼𝑛𝑓𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛𝐺𝑎𝑖𝑛𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑐𝑎𝑡𝑖𝑜𝑛=𝐸(𝑑)–∑|𝑠||𝑑|𝐸(𝑠)\n",
        "## 𝐼𝑛𝑓𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛𝐺𝑎𝑖𝑛𝑅𝑒𝑔𝑟𝑒𝑠𝑖𝑜𝑛=𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒(𝑑)–∑|𝑠||𝑑|𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒(𝑠)\n",
        "\n",
        "So the Information Gain will look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8aJFmBS_Emt"
      },
      "source": [
        "def variance(y):\n",
        "  '''\n",
        "  Function to help calculate the variance avoiding nan.\n",
        "  y: variable to calculate variance to. It should be a Pandas Series.\n",
        "  '''\n",
        "  if(len(y) == 1):\n",
        "    return 0\n",
        "  else:\n",
        "    return y.var()\n",
        "\n",
        "def information_gain(y, mask, func=entropy):\n",
        "  '''\n",
        "  It returns the Information Gain of a variable given a loss function.\n",
        "  y: target variable.\n",
        "  mask: split choice.\n",
        "  func: function to be used to calculate Information Gain in case os classification.\n",
        "  '''\n",
        "  \n",
        "  a = sum(mask)\n",
        "  b = mask.shape[0] - a\n",
        "  \n",
        "  if(a == 0 or b ==0): \n",
        "    ig = 0\n",
        "  \n",
        "  else:\n",
        "    if y.dtypes != 'O':\n",
        "      ig = variance(y) - (a/(a+b)* variance(y[mask])) - (b/(a+b)*variance(y[-mask]))\n",
        "    else:\n",
        "      ig = func(y)-a/(a+b)*func(y[mask])-b/(a+b)*func(y[-mask])\n",
        "  \n",
        "  return ig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4m9flOD_Ir5",
        "outputId": "916143f9-1b67-4709-8950-0c2efed7c3b0"
      },
      "source": [
        "##Now, we can calculate the information gain of a specific cut:\n",
        "information_gain(data['obese'], data['Gender'] == 'Male')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0002808244603327431"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBByDMf0AUfK"
      },
      "source": [
        "**Knowing this, the steps that we need to follow in order to code a decision tree from scratch in Python are simple:**\n",
        "\n",
        "1.Calculate the Information Gain for all variables.\n",
        "\n",
        "2.Choose the split that generates the highest Information Gain as a split.\n",
        "\n",
        "3.Repeat the process until at least one of the conditions set by hyperparameters of the algorithm is not fulfilled.\n",
        "\n",
        "**However, we have a newly added difficulty, and it is, how do we choose which is the best split in the numerical variables? And if there is more than one categorical variable?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHWeB0-CAXpk"
      },
      "source": [
        "# **How to calculate the best split for a variable**\n",
        "To calculate the best split of a numeric variable, first, all possible values that the variable is taking must be obtained. Once we have the options, for each option we will calculate the Information Gain using as a filter if the value is less than that value. Obviously, the first possible data will be drop, because the split will include all values.\n",
        "\n",
        "In case we have categorical variables, the idea is the same, only that in this case we will have to calculate the Information Gain for all possible combinations of that variable, excluding the option that includes all the options (since it would not be doing any split). This is quite computationally costly if we have a high number of categories, that decision tree algorithms usually only accept categorical variables with less than 20 categories.\n",
        "\n",
        "So, once we have all the splits, we will stick with the split that generates the highest Information Gain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGLHyHCTAgpp",
        "outputId": "59a8f2ca-2837-4c36-d261-40a64101f62c"
      },
      "source": [
        "import itertools\n",
        "\n",
        "def categorical_options(a):\n",
        "  '''\n",
        "  Creates all possible combinations from a Pandas Series.\n",
        "  a: Pandas Series from where to get all possible combinations. \n",
        "  '''\n",
        "  a = a.unique()\n",
        "\n",
        "  opciones = []\n",
        "  for L in range(0, len(a)+1):\n",
        "      for subset in itertools.combinations(a, L):\n",
        "          subset = list(subset)\n",
        "          opciones.append(subset)\n",
        "\n",
        "  return opciones[1:-1]\n",
        "\n",
        "def max_information_gain_split(x, y, func=entropy):\n",
        "  '''\n",
        "  Given a predictor & target variable, returns the best split, the error and the type of variable based on a selected cost function.\n",
        "  x: predictor variable as Pandas Series.\n",
        "  y: target variable as Pandas Series.\n",
        "  func: function to be used to calculate the best split.\n",
        "  '''\n",
        "\n",
        "  split_value = []\n",
        "  ig = [] \n",
        "\n",
        "  numeric_variable = True if x.dtypes != 'O' else False\n",
        "\n",
        "  # Create options according to variable type\n",
        "  if numeric_variable:\n",
        "    options = x.sort_values().unique()[1:]\n",
        "  else: \n",
        "    options = categorical_options(x)\n",
        "\n",
        "  # Calculate ig for all values\n",
        "  for val in options:\n",
        "    mask =   x < val if numeric_variable else x.isin(val)\n",
        "    val_ig = information_gain(y, mask, func)\n",
        "    # Append results\n",
        "    ig.append(val_ig)\n",
        "    split_value.append(val)\n",
        "\n",
        "  # Check if there are more than 1 results if not, return False\n",
        "  if len(ig) == 0:\n",
        "    return(None,None,None, False)\n",
        "\n",
        "  else:\n",
        "  # Get results with highest IG\n",
        "    best_ig = max(ig)\n",
        "    best_ig_index = ig.index(best_ig)\n",
        "    best_split = split_value[best_ig_index]\n",
        "    return(best_ig,best_split,numeric_variable, True)\n",
        "\n",
        "\n",
        "weight_ig, weight_slpit, _, _ = max_information_gain_split(data['Weight'], data['obese'],)  \n",
        "\n",
        "\n",
        "print(\n",
        "  \"The best split for Weight is when the variable is less than \",\n",
        "  weight_slpit,\"\\nInformation Gain for that split is:\", weight_ig\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best split for Weight is when the variable is less than  103 \n",
            "Information Gain for that split is: 0.10625190497954848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8GzkMGaAn-O"
      },
      "source": [
        "Now that we know how to calculate the split of a variable, let’s see how to decide the best split.\n",
        "\n",
        "# **How to choose the best split**\n",
        "As I have previously said, the best split will be the one that generates the highest Information Gain. To know which one is it, we simply have to calculate the Information Gain for each of the predictor variables of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "33pcd5laArIp",
        "outputId": "5925d65c-eea7-437c-a684-1a49c8b19ebb"
      },
      "source": [
        "data.drop('obese', axis= 1).apply(max_information_gain_split, y = data['obese'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.000280824</td>\n",
              "      <td>0.0196836</td>\n",
              "      <td>0.106252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Male]</td>\n",
              "      <td>174</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Gender     Height    Weight\n",
              "0 -0.000280824  0.0196836  0.106252\n",
              "1       [Male]        174       103\n",
              "2        False       True      True\n",
              "3         True       True      True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaPOdLxJAvRb"
      },
      "source": [
        "As we can see, the variable with the highest Information Gain is Weight. Therefore, it will be the variable that we use first to do the split. In addition, we also have the value on which the split must be performed: 103.\n",
        "\n",
        "With this, we already have the first split, which would generate two dataframes. If we apply this recursively, we will end up creating the entire decision tree (coded in Python from scratch). Let’s do it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYj3ryz8A-w3"
      },
      "source": [
        "# **How to train a decision tree in Python from scratch**\n",
        "# **Determining the depth of the tree**\n",
        "We already have all the ingredients to calculate our decision tree. Now, we must create a function that, given a mask, makes us a split.\n",
        "\n",
        "In addition, we will include the different hyperparameters that a decision tree generally offers. Although we could include more, the most relevant are those that prevent the tree from growing too much, thus avoiding overfitting. These hyperparameters are as follows:\n",
        "\n",
        "max_depth: maximum depth of the tree. If we set it to None, the tree will grow until all the leaves are pure or the hyperparameter min_samples_split has been reached.\n",
        "\n",
        "min_samples_split: indicates the minimum number of observations a sheet must have to continue creating new nodes.\n",
        "\n",
        "min_information_gain: the minimum amount the Information Gain must increase for the tree to continue growing.\n",
        "\n",
        "With this in mind, let’s finish creating our decision tree from 0 in Python. To do this, we will:\n",
        "\n",
        "1.Make sure that the conditions established by min_samples_split and max_depth are being fulfilled.\n",
        "\n",
        "2.Make the split.\n",
        "\n",
        "3.Ensure that min_information_gain if fulfilled.\n",
        "\n",
        "4.Save the data of the split and repeat the process.\n",
        "\n",
        "To do this, first of all, I will create three functions: one that, given some data, returns the best split with its corresponding information, another that, given some data and a split, makes the split and returns the prediction and finally, a function that given some data, makes a prediction.\n",
        "\n",
        "**Note: the prediction will only be given in the branches and basically consists of returning the mean of the data in the case of the regression or the mode in the case of the classification.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk5rhsYoBTcT"
      },
      "source": [
        "def get_best_split(y, data):\n",
        "  '''\n",
        "  Given a data, select the best split and return the variable, the value, the variable type and the information gain.\n",
        "  y: name of the target variable\n",
        "  data: dataframe where to find the best split.\n",
        "  '''\n",
        "  masks = data.drop(y, axis= 1).apply(max_information_gain_split, y = data[y])\n",
        "  if sum(masks.loc[3,:]) == 0:\n",
        "    return(None, None, None, None)\n",
        "\n",
        "  else:\n",
        "    # Get only masks that can be splitted\n",
        "    masks = masks.loc[:,masks.loc[3,:]]\n",
        "\n",
        "    # Get the results for split with highest IG\n",
        "    split_variable = max(masks)\n",
        "    #split_valid = masks[split_variable][]\n",
        "    split_value = masks[split_variable][1] \n",
        "    split_ig = masks[split_variable][0]\n",
        "    split_numeric = masks[split_variable][2]\n",
        "\n",
        "    return(split_variable, split_value, split_ig, split_numeric)\n",
        "\n",
        "\n",
        "def make_split(variable, value, data, is_numeric):\n",
        "  '''\n",
        "  Given a data and a split conditions, do the split.\n",
        "  variable: variable with which make the split.\n",
        "  value: value of the variable to make the split.\n",
        "  data: data to be splitted.\n",
        "  is_numeric: boolean considering if the variable to be splitted is numeric or not.\n",
        "  '''\n",
        "  if is_numeric:\n",
        "    data_1 = data[data[variable] < value]\n",
        "    data_2 = data[(data[variable] < value) == False]\n",
        "\n",
        "  else:\n",
        "    data_1 = data[data[variable].isin(value)]\n",
        "    data_2 = data[(data[variable].isin(value)) == False]\n",
        "\n",
        "  return(data_1,data_2)\n",
        "\n",
        "def make_prediction(data, target_factor):\n",
        "  '''\n",
        "  Given the target variable, make a prediction.\n",
        "  data: pandas series for target variable\n",
        "  target_factor: boolean considering if the variable is a factor or not\n",
        "  '''\n",
        "\n",
        "  # Make predictions\n",
        "  if target_factor:\n",
        "    pred = data.value_counts().idxmax()\n",
        "  else:\n",
        "    pred = data.mean()\n",
        "\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi40q860B8-d"
      },
      "source": [
        "Training our decision tree in Python\n",
        "Now that we have these three functions, we can, let’s train the decision tree that we just programmed in Python.\n",
        "\n",
        "1. We ensure that both min_samples_split and max_depth are fulfilled.\n",
        "\n",
        "2. If they are fulfilled, we get the best split and obtain the Information Gain. If any of the conditions are not fulfilled, we make the prediction.\n",
        "\n",
        "3. We check that the Information Gain Comprobamos passes the minimum amount set by min_information_gain.\n",
        "\n",
        "4. If the condition above is fulfilled, we make the split and save the decision. If it is not fulfilled, then we make the prediction.\n",
        "\n",
        "We will do this process recursively, that is, the function will call itself. The result of the function will be the rules you follow to make the decision:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZHWjGP9CN6i",
        "outputId": "d17460ba-1e1d-4b25-e7fd-24ab46f84457"
      },
      "source": [
        "def train_tree(data,y, target_factor, max_depth = None,min_samples_split = None, min_information_gain = 1e-20, counter=0, max_categories = 20):\n",
        "  '''\n",
        "  Trains a Decission Tree\n",
        "  data: Data to be used to train the Decission Tree\n",
        "  y: target variable column name\n",
        "  target_factor: boolean to consider if target variable is factor or numeric.\n",
        "  max_depth: maximum depth to stop splitting.\n",
        "  min_samples_split: minimum number of observations to make a split.\n",
        "  min_information_gain: minimum ig gain to consider a split to be valid.\n",
        "  max_categories: maximum number of different values accepted for categorical values. High number of values will slow down learning process. R\n",
        "  '''\n",
        "\n",
        "  # Check that max_categories is fulfilled\n",
        "  if counter==0:\n",
        "    types = data.dtypes\n",
        "    check_columns = types[types == \"object\"].index\n",
        "    for column in check_columns:\n",
        "      var_length = len(data[column].value_counts()) \n",
        "      if var_length > max_categories:\n",
        "        raise ValueError('The variable ' + column + ' has '+ str(var_length) + ' unique values, which is more than the accepted ones: ' +  str(max_categories))\n",
        "\n",
        "  # Check for depth conditions\n",
        "  if max_depth == None:\n",
        "    depth_cond = True\n",
        "\n",
        "  else:\n",
        "    if counter < max_depth:\n",
        "      depth_cond = True\n",
        "\n",
        "    else:\n",
        "      depth_cond = False\n",
        "\n",
        "  # Check for sample conditions\n",
        "  if min_samples_split == None:\n",
        "    sample_cond = True\n",
        "\n",
        "  else:\n",
        "    if data.shape[0] > min_samples_split:\n",
        "      sample_cond = True\n",
        "\n",
        "    else:\n",
        "      sample_cond = False\n",
        "\n",
        "  # Check for ig condition\n",
        "  if depth_cond & sample_cond:\n",
        "\n",
        "    var,val,ig,var_type = get_best_split(y, data)\n",
        "\n",
        "    # If ig condition is fulfilled, make split \n",
        "    if ig is not None and ig >= min_information_gain:\n",
        "\n",
        "      counter += 1\n",
        "\n",
        "      left,right = make_split(var, val, data,var_type)\n",
        "\n",
        "      # Instantiate sub-tree\n",
        "      split_type = \"<=\" if var_type else \"in\"\n",
        "      question =   \"{} {}  {}\".format(var,split_type,val)\n",
        "      # question = \"\\n\" + counter*\" \" + \"|->\" + var + \" \" + split_type + \" \" + str(val) \n",
        "      subtree = {question: []}\n",
        "\n",
        "\n",
        "      # Find answers (recursion)\n",
        "      yes_answer = train_tree(left,y, target_factor, max_depth,min_samples_split,min_information_gain, counter)\n",
        "\n",
        "      no_answer = train_tree(right,y, target_factor, max_depth,min_samples_split,min_information_gain, counter)\n",
        "\n",
        "      if yes_answer == no_answer:\n",
        "        subtree = yes_answer\n",
        "\n",
        "      else:\n",
        "        subtree[question].append(yes_answer)\n",
        "        subtree[question].append(no_answer)\n",
        "\n",
        "    # If it doesn't match IG condition, make prediction\n",
        "    else:\n",
        "      pred = make_prediction(data[y],target_factor)\n",
        "      return pred\n",
        "\n",
        "   # Drop dataset if doesn't match depth or sample conditions\n",
        "  else:\n",
        "    pred = make_prediction(data[y],target_factor)\n",
        "    return pred\n",
        "\n",
        "  return subtree\n",
        "\n",
        "\n",
        "max_depth = 5\n",
        "min_samples_split = 20\n",
        "min_information_gain  = 1e-5\n",
        "\n",
        "\n",
        "decisiones = train_tree(data,'obese',True, max_depth,min_samples_split,min_information_gain)\n",
        "\n",
        "\n",
        "decisiones"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Weight <=  103': [{'Weight <=  74': [0,\n",
              "    {'Weight <=  84': [{'Weight <=  75': [1, 0]},\n",
              "      {'Weight <=  98': [1, 0]}]}]},\n",
              "  1]}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiOBPCoOCTFj"
      },
      "source": [
        "**It is done! The decision tree we just coded in Python has created all the rules that it will use to make predictions.**\n",
        "\n",
        "Now, there would only be one thing left: convert those rules into concrete actions that the algorithm can use to classify new data. Let’s go for it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8jg7fu4CX_Q"
      },
      "source": [
        "# Predict using our decision tree in Python\n",
        "To make the prediction, we are going to take an observation and the decision tree. These decisions can be converted into real conditions by splitting them.\n",
        "\n",
        "So, to make the prediction we are going to:\n",
        "\n",
        "1. Break the decision into several chunks.\n",
        "\n",
        "2. Check the type of decision that it is (numerical or categorical).\n",
        "\n",
        "3. Considering the type of variable that it is, check the decision boundary. If the decision is fulfilled, return the result, if it is not, then continue with the decision.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N4Fq8LxCkJ3"
      },
      "source": [
        "def clasificar_datos(observacion, arbol):\n",
        "  question = list(arbol.keys())[0] \n",
        "\n",
        "  if question.split()[1] == '<=':\n",
        "\n",
        "    if observacion[question.split()[0]] <= float(question.split()[2]):\n",
        "      answer = arbol[question][0]\n",
        "    else:\n",
        "      answer = arbol[question][1]\n",
        "\n",
        "  else:\n",
        "\n",
        "    if observacion[question.split()[0]] in (question.split()[2]):\n",
        "      answer = arbol[question][0]\n",
        "    else:\n",
        "      answer = arbol[question][1]\n",
        "\n",
        "  # If the answer is not a dictionary\n",
        "  if not isinstance(answer, dict):\n",
        "    return answer\n",
        "  else:\n",
        "    residual_tree = answer\n",
        "    return clasificar_datos(observacion, answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmZ6mHPOCpNs"
      },
      "source": [
        "So, we can try to classify all the data in our algorithm to see how well our decision tree has worked that we just programmed in Python:"
      ]
    }
  ]
}